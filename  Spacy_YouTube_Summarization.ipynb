{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "video=input(\"Enter the link of your YouTube Video: \")\n",
    "id_video=video.split(\"=\")[1]\n",
    "print(id_video)\n",
    "\n",
    "YouTubeVideo(id_video)\n",
    "transcript = YouTubeTranscriptApi.get_transcript(id_video)\n",
    "transcript\n",
    "doc = \"\"\n",
    "for line in transcript:\n",
    "    doc =doc+ ' ' + line['text']\n",
    "print(type(doc))\n",
    "print(doc)\n",
    "#print(len(result))\n",
    "doc=[]\n",
    "for line in transcript:\n",
    "  if \"\\n\" in line['text']:\n",
    "    x=line['text'].replace(\"\\n\",\" \")\n",
    "    doc.append(x)\n",
    "  else:\n",
    "    doc.append(line['text'])\n",
    "print(doc)\n",
    "\n",
    "paragraph=\" \".join(doc)\n",
    "print(paragraph)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "mytext= paragraph\n",
    "stops = set(stopwords.words('english'))\n",
    "word_array = word_tokenize(mytext)\n",
    "\n",
    "wordfreq=dict()\n",
    "for word in word_array:\n",
    "  word=word.lower()\n",
    "  if word in stops:\n",
    "    continue\n",
    "  elif word in wordfreq:\n",
    "    wordfreq[word]+=1\n",
    "  else:\n",
    "    wordfreq[word]=1\n",
    "\n",
    "#word_array\n",
    "#frequencytable\n",
    "sent_array=sent_tokenize(mytext)\n",
    "\n",
    "sentfreq=dict()\n",
    "for sentence in sent_array:\n",
    "  for word,freq in wordfreq.items():\n",
    "    if word in sentence.lower():\n",
    "      if sentence in sentfreq:\n",
    "        sentfreq[sentence]+=freq\n",
    "      else:\n",
    "        sentfreq[sentence]=freq  \n",
    "\n",
    "#sentfreq\n",
    "\n",
    "averageval=0\n",
    "for sentence in sentfreq:\n",
    "  averageval+=sentfreq[sentence]\n",
    "\n",
    "average=int(averageval/len(sentfreq))\n",
    "summary=''\n",
    "for sentence in sent_array:\n",
    "  if(sentence in sentfreq) and (sentfreq[sentence]>(1.5*average)):\n",
    "    summary=summary+\" \"+sentence\n",
    "print(summary)\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "stopwords=list(STOP_WORDS)\n",
    "from string import punctuation\n",
    "punctuation=punctuation+ '\\n'\n",
    "#punctuation\n",
    "\n",
    "text=paragraph\n",
    "space = spacy.load('en_core_web_sm')\n",
    "doc= space(text)\n",
    "word_frequencies={}\n",
    "for word in doc:\n",
    "  if word.text.lower() not in stopwords:\n",
    "      if word.text.lower() not in punctuation:\n",
    "          if word.text not in word_frequencies.keys():\n",
    "              word_frequencies[word.text] = 1\n",
    "          else:\n",
    "              word_frequencies[word.text] += 1\n",
    "\n",
    "\n",
    "max_frequency=max(word_frequencies.values())\n",
    "for word in word_frequencies.keys():\n",
    "  word_frequencies[word]=word_frequencies[word]/max_frequency\n",
    "  sentence_tokens= [sent for sent in doc.sents]\n",
    "sentence_scores = {}\n",
    "for sent in sentence_tokens:\n",
    "  for word in sent:\n",
    "      if word.text.lower() in word_frequencies.keys():\n",
    "          if sent not in sentence_scores.keys():                            \n",
    "            sentence_scores[sent]=word_frequencies[word.text.lower()]\n",
    "          else:\n",
    "            sentence_scores[sent]+=word_frequencies[word.text.lower()]\n",
    "#sentence_scores  \n",
    "percent=int(input(\"How much percentage of summary you want? \"))\n",
    "ratio=(int(percent)) / 100\n",
    "#ratio\n",
    "\n",
    "from heapq import nlargest\n",
    "select_length=int(len(sentence_tokens)*ratio)\n",
    "select_length\n",
    "summary=nlargest(select_length, sentence_scores,key=sentence_scores.get)\n",
    "summary\n",
    "     \n",
    "final_summary=[word.text for word in summary]\n",
    "final_summary\n",
    "summary=''.join(final_summary)\n",
    "summary\n",
    "\n",
    "     \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
