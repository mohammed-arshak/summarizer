{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from IPython.display import YouTubeVideo\n",
    "     \n",
    "video=input(\"Enter the link of your YouTube Video: \")\n",
    "id_video=video.split(\"=\")[1]\n",
    "print(id_video)\n",
    "     \n",
    "\n",
    "YouTubeVideo(id_video)\n",
    "transcript = YouTubeTranscriptApi.get_transcript(id_video)\n",
    "transcript\n",
    "\n",
    "doc = \"\"\n",
    "for line in transcript:\n",
    "    doc =doc+ ' ' + line['text']\n",
    "print(type(doc))\n",
    "print(doc)\n",
    "#print(len(result))\n",
    "\n",
    "doc=[]\n",
    "for line in transcript:\n",
    "  if \"\\n\" in line['text']:\n",
    "    x=line['text'].replace(\"\\n\",\" \")\n",
    "    doc.append(x)\n",
    "  else:\n",
    "    doc.append(line['text'])\n",
    "print(doc)\n",
    "\n",
    "paragraph=\" \".join(doc)\n",
    "print(paragraph)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "mytext= paragraph\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "word_array = word_tokenize(mytext)\n",
    "\n",
    "wordfreq=dict()\n",
    "for word in word_array:\n",
    "  word=word.lower()\n",
    "  if word in stops:\n",
    "    continue\n",
    "  elif word in wordfreq:\n",
    "    wordfreq[word]+=1\n",
    "  else:\n",
    "    wordfreq[word]=1\n",
    "\n",
    "#word_array\n",
    "#frequencytable\n",
    "\n",
    "sent_array=sent_tokenize(mytext)\n",
    "\n",
    "sentfreq=dict()\n",
    "for sentence in sent_array:\n",
    "  for word,freq in wordfreq.items():\n",
    "    if word in sentence.lower():\n",
    "      if sentence in sentfreq:\n",
    "        sentfreq[sentence]+=freq\n",
    "      else:\n",
    "        sentfreq[sentence]=freq  \n",
    "\n",
    "#sentfreq\n",
    "averageval=0\n",
    "for sentence in sentfreq:\n",
    "  averageval+=sentfreq[sentence]\n",
    "\n",
    "average=int(averageval/len(sentfreq))\n",
    "summary=''\n",
    "for sentence in sent_array:\n",
    "  if(sentence in sentfreq) and (sentfreq[sentence]>(1.5*average)):\n",
    "    summary=summary+\" \"+sentence\n",
    "print(summary)\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "stopwords=list(STOP_WORDS)\n",
    "from string import punctuation\n",
    "punctuation=punctuation+ '\\n'\n",
    "#punctuation\n",
    "text=paragraph\n",
    "space = spacy.load('en_core_web_sm')\n",
    "doc= space(text)\n",
    "word_frequencies={}\n",
    "for word in doc:\n",
    "  if word.text.lower() not in stopwords:\n",
    "      if word.text.lower() not in punctuation:\n",
    "          if word.text not in word_frequencies.keys():\n",
    "              word_frequencies[word.text] = 1\n",
    "          else:\n",
    "              word_frequencies[word.text] += 1\n",
    "max_frequency=max(word_frequencies.values())\n",
    "for word in word_frequencies.keys():\n",
    "  word_frequencies[word]=word_frequencies[word]/max_frequency\n",
    "  sentence_tokens= [sent for sent in doc.sents]\n",
    "sentence_scores = {}\n",
    "for sent in sentence_tokens:\n",
    "  for word in sent:\n",
    "      if word.text.lower() in word_frequencies.keys():\n",
    "          if sent not in sentence_scores.keys():                            \n",
    "            sentence_scores[sent]=word_frequencies[word.text.lower()]\n",
    "          else:\n",
    "            sentence_scores[sent]+=word_frequencies[word.text.lower()]\n",
    "#sentence_scores  \n",
    "     \n",
    "percent=int(input(\"How much percentage of summary you want? \"))\n",
    "ratio=(int(percent)) / 100\n",
    "#ratio\n",
    "\n",
    "from heapq import nlargest\n",
    "select_length=int(len(sentence_tokens)*ratio)\n",
    "select_length\n",
    "summary=nlargest(select_length, sentence_scores,key=sentence_scores.get)\n",
    "summary\n",
    "final_summary=[word.text for word in summary]\n",
    "final_summary\n",
    "summary=''.join(final_summary)\n",
    "summary\n",
    "import googletrans\n",
    "\n",
    "\n",
    "from deep_translator import GoogleTranslator\n",
    "from deep_translator import MicrosoftTranslator\n",
    "\n",
    "print(\"CHECK OUT YOUR SOURCE LANGUAGE FROM BELOW:\")\n",
    "print(\" \")\n",
    "langdict=googletrans.LANGUAGES\n",
    "for i in langdict :\n",
    "    print(i+\"-\" +langdict[i])\n",
    "srclang=input(\"ENTER LANGUAGE CODE: \")\n",
    "to_translate = summary\n",
    "translated = GoogleTranslator(source='auto', target=srclang).translate(to_translate)\n",
    "print(\"YOUR TRANSLATED SUMMARY IS GIVEN BELOW:\")\n",
    "print(\" \")\n",
    "print(translated)\n",
    "import gtts\n",
    "\n",
    "print(\"Choose the below option:\")\n",
    "print(\" \")\n",
    "print(\"E for converting extracted summary to speech.\\n\" \"T for converting translated summary to speech.\\n\" \"B for converting both translated and extracted summary to speech.\")\n",
    "option=input(\"Enter your choice: \")\n",
    "if option=='E' or option=='e':\n",
    "  speechtext=summary\n",
    "  speech1=gtts.gTTS(speechtext)\n",
    "  name1=input(\"Enter name of Extracted Summary file: \")\n",
    "  speech1.save(name1+\".mp3\")\n",
    "\n",
    "elif option=='T' or option=='t':\n",
    "  speechtext=translated\n",
    "  speech1=gtts.gTTS(speechtext)\n",
    "  name2=input(\"Enter name of Translated Summary file: \")\n",
    "  speech1.save(name2+\".mp3\")\n",
    "\n",
    "elif option=='B' or option=='b':\n",
    "  speechtext1=summary\n",
    "  speechtext2=translated\n",
    "  speech1=gtts.gTTS(speechtext1)\n",
    "  speech2=gtts.gTTS(speechtext2)\n",
    "  name1=input(\"Enter name of Extracted Summary file: \")\n",
    "  name2=input(\"Enter name of Translated Summary file: \")\n",
    "  speech1.save(name1+\".mp3\")\n",
    "  speech2.save(name2+\".mp3\")\n",
    "\n",
    "else:\n",
    "  print(\"Invalid Option\")\n",
    "\n",
    "from transformers import pipeline\n",
    "summarizer = pipeline('summarization')\n",
    "result=paragraph\n",
    "\n",
    "num_iters = int(len(result)/1000)\n",
    "summarized_text = []\n",
    "for i in range(0, num_iters + 1):\n",
    "  start = 0\n",
    "  start = i * 1000\n",
    "  end = (i + 1) * 1000\n",
    "  print(\"input text \\n\" + result[start:end])\n",
    "  out = summarizer(result[start:end])\n",
    "  out = out[0]\n",
    "  out = out['summary_text']\n",
    "  print(\"Summarized text\\n\"+out)\n",
    "  summarized_text.append(out)\n",
    "\n",
    "#print(summarized_text)\n",
    "mysummary=\" \".join(summarized_text)\n",
    "print(mysummary)\n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
